{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a simple version of the knapsack problem, where each item is associated with a value and the task is to choose a subset of the items that maximizes the sum of the values of the items. We assume there are 10 items with the same weight 2, and the capacity of the knapsack is 15. For example,\n",
    "\n",
    "    [2,7,3,5,2,3,8,2,1,5][1,2,3,4,5,6,9]\n",
    "\n",
    "is a labeled example such that the first list specifies the values of the 10 items and the second list is a solution that specifies the indices of the items to be put into the knapsack. Since the capacity of the knapsack is fixed to be 15 and each item has weight 2, one can infer that the solutions always contain 7 items.\n",
    "## Data Format\n",
    "\n",
    "In dataGen.py, a class named \"KsData\" is defined in the following way.\n",
    "\n",
    "KsData class has 6 attributes: train_data, test_data, valid_data, train_labels, test_labels, valid_labels.\n",
    "\n",
    "train_data is an numpy array of size (1800, 10). It consists of 1800 data as follows \n",
    "\n",
    "        [\n",
    "          data,\n",
    "          ...,\n",
    "          data\n",
    "        ]\n",
    "        \n",
    "where data is a vector (numpy array) of length 10. For example, the data shown below  \n",
    "\n",
    "        [2 2 1 3 1 2 8 1 5 1]\n",
    "        \n",
    "defines the 10 values of the 10 items.  \n",
    "train_labels is an numpy array of size (1800, 10). It consists of 1800 label as follows.  \n",
    "\n",
    "        [\n",
    "          label,\n",
    "          ...,\n",
    "          label\n",
    "        ]\n",
    "\n",
    "where label is a vector (numpy array) of length 10, with k \"1\" and (10-k) \"0\". For example, the label shown below  \n",
    "\n",
    "        [0 0 1 0 0 0 0 0 0 0]\n",
    "\n",
    "means that the item 2 is chosen to be put into the knapsack.  \n",
    "test_data is a numpy array of size (600, 10).   \n",
    "valid_data is a numpy array of size (600, 10).  \n",
    "test_labels is a numpy array of size (600, 10).  \n",
    "valid_labels is a numpy array of size (600, 10).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "from dataGen import KsData\n",
    "from neurasp import NeurASP\n",
    "from network import FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeurASP Program for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprogram='''\n",
    "% define maxweight k \n",
    "#const k = 7.\n",
    "\n",
    "nn(in(10, k), [true, false]).\n",
    "\n",
    "% we make a mistake if the total weight of the chosen items exceeds maxweight \n",
    ":- #sum{1, I : in(k,I,true)} > k.\n",
    "'''\n",
    "\n",
    "dprogram_test='''\n",
    "% define maxweight k \n",
    "#const k = 7.\n",
    "\n",
    "% we make a mistake if the total weight of the chosen items exceeds maxweight \n",
    ":- #sum{1, I : in(k,I,true)} > k.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Instantiation\n",
    "- Instantiate neural networks.\n",
    "- Define nnMapping: a dictionary that maps neural network names (i.e., strings) to the neural network objects (i.e., torch.nn.Module object)\n",
    "- Define optimizers: a dictionary that specifies the optimizer for each network (we use the Adam optimizer here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network (MLP) Structure: (10, 50, 50, 50, 50, 50, 10)\n"
     ]
    }
   ],
   "source": [
    "m = FC(10, *[50, 50, 50, 50, 50], 10)\n",
    "nnMapping = {'in': m}\n",
    "optimizer = {'in':torch.optim.Adam(m.parameters(), lr=0.001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NeurASP Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeurASPobj = NeurASP(dprogram, nnMapping, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataList and obsList for Training, testDataList and testObsList for Testing\n",
    "### Create the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KsData(\"data/data.txt\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct dataList and obsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = []\n",
    "obsList = []\n",
    "for i, d in enumerate(dataset.train_data):\n",
    "    d_tensor = Variable(torch.from_numpy(d).float(), requires_grad=False)\n",
    "    dataList.append({'k': d_tensor})\n",
    "with open('data/evidence_train.txt', 'r') as f:\n",
    "    obsList = f.read().strip().strip('#evidence').split('#evidence')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct testDataList and testObsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = []\n",
    "testObsLost = []\n",
    "for d in dataset.test_data:\n",
    "    d_tensor = Variable(torch.from_numpy(d).float(), requires_grad=False)\n",
    "    testData.append({'k': d_tensor})\n",
    "with open('data/evidence_test.txt', 'r') as f:\n",
    "    testObsLost = f.read().strip().strip('#evidence').split('#evidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "Note that our target is to find the set of items with maximal sum of the values, which is represented by the optimal stable models of the logic program. To find the optimal stable models instead of stable models during training, we need to specify \"opt=True\" in the learning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n",
      "The accuracy for constraint 1 is 0.13333333333333333\n",
      "--- train time: 18.8836829662323 seconds ---\n",
      "--- test time: 1.6695325374603271 seconds ---\n",
      "--- total time from beginning: 0 minutes ---\n",
      "Epoch 2...\n",
      "The accuracy for constraint 1 is 0.21666666666666667\n",
      "--- train time: 16.198837995529175 seconds ---\n",
      "--- test time: 1.7106995582580566 seconds ---\n",
      "--- total time from beginning: 0 minutes ---\n",
      "Epoch 3...\n",
      "The accuracy for constraint 1 is 0.34\n",
      "--- train time: 16.13485550880432 seconds ---\n",
      "--- test time: 1.663581371307373 seconds ---\n",
      "--- total time from beginning: 0 minutes ---\n",
      "Epoch 4...\n",
      "The accuracy for constraint 1 is 0.3933333333333333\n",
      "--- train time: 15.817415237426758 seconds ---\n",
      "--- test time: 1.7325234413146973 seconds ---\n",
      "--- total time from beginning: 1 minutes ---\n",
      "Epoch 5...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fa1748891063>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {}...'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtime1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mNeurASPobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataList\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobsList\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobsList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmPickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/stableModels.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtime2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mNeurASPobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtestConstraint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestObsLost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdprogram_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\NeurASP\\neurasp.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, dataList, obsList, epoch, alpha, lossFunc, method, lr, opt, storeSM, smPickle, accEpoch, batchSize)\u001b[0m\n\u001b[0;32m    271\u001b[0m                             \u001b[0mdataTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m                         \u001b[0mnnOutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m                         \u001b[0mnnOutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnOutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10e-8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "startTime = time.time()\n",
    "for i in range(200):\n",
    "    print('Epoch {}...'.format(i+1))\n",
    "    time1 = time.time()\n",
    "    NeurASPobj.learn(dataList=dataList, obsList=obsList, epoch=1, alpha=0, opt=True, smPickle='data/stableModels.pickle')\n",
    "    time2 = time.time()\n",
    "    NeurASPobj.testConstraint(testData, testObsLost,[dprogram_test])\n",
    "    print('--- train time: %s seconds ---' % (time2 - time1))\n",
    "    print('--- test time: %s seconds ---' % (time.time() - time2))\n",
    "    print('--- total time from beginning: %s minutes ---' % int((time.time() - startTime)/60) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
